{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando PPL de las transcripciones\n",
    "\n",
    "Para leer sobre la perplejidad consultar los siguientes links: \n",
    "https://huggingface.co/spaces/evaluate-metric/perplexity \n",
    "https://huggingface.co/docs/transformers/perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"openai-community/gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5606\n",
      " Buenas tardes, el día de hoy vamos a presentar el trabajo final de estadística vallisiana por Nicolás Gil Lugo y mi persona Laura Lucía Domínguez. Como contenido tenemos la introducción, problema de investigación, formas analíticas de las distribuciones, estimaciones e indicadores, conclusiones y la bibliografía. Muy buenas tardes, como introducción entonces tenemos que en este trabajo se aborda un problema específico desde el enfoque vallisiano para la estimación de un parámetro que para nosotros es de interés. El punto de vista vallisiano considera un parámetro desconocido como una característica con respecto a la cual puede expresarse un grado de credibilidad la cual puede ser modificada mediante la información muestral. El problema de investigación. Es que un médico oncólogo desea analizar los niveles de tiroglobulina en un grupo de 35 indebidos con cáncer de tiroides después de un año de ser sometidos a radioterapia con yodo. Y este tratamiento es exitoso si los niveles de tiroglobulina en la sangre disminuyen a niveles de 2 nanogramos por mililitro o menos. Entonces tenemos el modelo estadístico el cual está el proceso generador de los datos. Es el tratamiento con radioterapia ya que modifica las condiciones en las que se encuentra el paciente. El indicador es el nivel de tiroglobulina ya que sobre este valor vamos a observar si mejoró o no el paciente. Comunidad de estudio tenemos los niveles de tiroglobulina en los pacientes después de ser sometidos a la terapia con radioterapia. La variable aleatoria se distribuye de manera gama con alfa y beta mayores que cero. Y para este caso vamos a asumir que la variable aleatoria es la que más se distribuye. El parámetro es la media de los niveles de tiroglobulina después de terminar el tratamiento. Se calcula ese alfa el cual estamos asumiendo conocido mediante el método de los momentos el cual da un resultado de 4.26. El proceso de licitación lo hicimos de acuerdo a un artículo por Tobar y Mejía el cual presentan el grupo completo sin recaída. El proceso de licitación lo hicimos de acuerdo a un artículo por Tobar y Mejía el cual presentan el grupo completo sin recaída. Y las mediciones las tomamos como intervalo de la mediana a la media y de la media al máximo. Así construimos cada intervalo y de cada intervalo obtuvimos la media para así obtener los parámetros de la distribución a priori. Bueno, como distribuciones, aclarando que el alfa es conocido y es de 4.26, tenemos la distribución a priori informativa, lo cual se distribuye de manera gama y sus parámetros, están denotados por alfa estrella, beta estrella. La distribución a posteriori que es la combinación de la verosimilitud de los datos muestrales con esa distribución a priori informativa, se distribuye de manera gama. También encontramos una distribución a priori con el método de Jeffrey, la cual es impropia, pero la posterior que se genera combinándola con la verosimilitud es propia y se distribuye de manera gama, al igual que las anteriormente mencionadas. La distribución predictiva, a priori predictiva, al combinar la verosimilitud con la a priori, pues nos da que no tiene forma. Y la posterior predictiva, al combinar la verosimilitud de una nueva observación, una sola observación, con la posteriori, también nos da que no tiene forma definida. Esto lo podríamos resolver con el algoritmo de Metropolis Hasting. En las estimaciones puntuales, tenemos la función de pérdida cuadrática, la función de pérdida absoluta, la región de credibilidad para el parámetro beta. También tenemos un resumen estadístico, el cual fue obtenido mediante el paquete Openbox. Podemos ver que para nuestro parámetro de interés, que es la esperanza, la media fue de 1.6 y el percentil del 97,5% es de 1.9, lo cual está por debajo del parámetro, del valor especificado de 2 ng por mililitro. Como test de hipótesis, vamos a realizarlo de acuerdo a nuestro parámetro de interés, el cual es la esperanza. Por ello, colocamos teta igual a alfa sobre beta. Y planteamos la hipótesis nula como teta mayor o igual que 2, y la hipótesis alterna como teta menor a 2. Lo que significaría que si rechazamos H0, estaríamos aceptando que los niveles de la hipótesis nula, que son los niveles de la hipótesis nula, estarían aceptando que los niveles de tiroglobulina en los pacientes son menores que 2. Como se observa en el gráfico, tenemos una prueba unilateral en la cual el valor crítico es de 2. Y al hacer Bayes Factor, tenemos que 207 de las veces es más probable que pertenezca a la región de rechazo o a la región de la hipótesis alterna. Por lo tanto, estaríamos concluyendo que el tratamiento sí es exitoso. Las personas sí tienen mejoría y sus niveles de tiroglobulina son menores a 2 ng por mililitro. Como conclusiones tenemos que al realizar la selección de modelos, el DIC y el Bayes Factor indican que el mejor modelo es el planteado con la distribución a priori no informativa, pero dicha distribución es impropia. Y las distribuciones predictivas, tanto a priori como posteriori, no presentan una forma de definición. Es por ello que debería realizarse el algoritmo de Metropolis-Hastings. También podemos decir que según los datos y el estudio realizado, el tratamiento sí es efectivo. Los niveles de tiroglobulina están siendo por debajo de 2 ng por mililitro, lo cual es satisfactorio para el estudio. Y se recomienda, tal vez, hablar con el oncólogo y decidir si para aquellos pacientes que se está pasando por muy poco, el valor de la distribución es relevante, ya que puede cambiar los resultados del estudio. Y la bibliografía. Muchas gracias.\n"
     ]
    }
   ],
   "source": [
    "with open('transcriptions/transcription_large.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    transcription = file.read()\n",
    "    print(len(transcription))\n",
    "    print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15886 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(transcription), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [03:37<00:15,  7.78s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.8597)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [04:05<00:16,  8.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.7032)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# large\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis_audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
